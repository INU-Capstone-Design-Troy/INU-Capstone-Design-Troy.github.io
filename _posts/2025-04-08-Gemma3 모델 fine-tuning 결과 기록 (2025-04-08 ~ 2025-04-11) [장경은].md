---
layout: post
title: "Gemma3 모델 fine-tuning 결과 기록 (2025-04-08 ~ 2025-04-11) [장경은]"
date: 2025-04-08 10:00:00 +0900
---

### 기존 개발 방식의 문제점 발견

- 기존 아이디어는 
  - OJ 상에 코딩 문제와 모범 코드를 업로드하면
  - 해당 모범 코드를 GPT-4o mini API를 이용하여 특정 단위에 의해서 분리하고 설명문을 제작하여 "가이드라인"을 제작한다. (여기서 가이드라인은 우리가 흔히 다는 "주석"처럼 내가 무엇을 해야하는지 방향을 제시하는 일종의 설명문이다.) 
  - 사용자는 해당 가이드라인에 따라서 설명문을 보고 코드를 한 줄씩 제출하면
  - OJ가 피드백을 제공한다는 시나리오였다.
- 그러나, 실제 테스트 후 **GPT-4o mini API는 우리가 원하는 프로토콜에 따라서 "항상" 원활하게 분리하지 못한다는 한계**를 보였다.
- 따라서 API 이용은 실제 상용화에 문제가 있다고 판단하여, fine-tuning으로 정형화된 가이드라인을 생성할 필요성을 느끼게 되었다.

---

### 변경된 개발 방식

- 간단히 말해서 **가이드라인을 제작하는 방식**을 변경하였다.

  1. 조교 또는 교수님께서 OJ 상에 코딩 문제와 모범 코드를 업로드한다.

  2. OJ는 모범 코드를 분리하는 전처리 코드를 실행하여, 단계에 맞는 기준에 따라 코드를 분리한다. (ex: 한 줄 단위, 단락 단위, 기능 단위)

     - 전처리 코드는 stack의 pop, push 원리로 작성되었다.

     ex: [func_start], [rep_start]

  3. 분리된 코드마다 해당 코드를 작성할 수 있도록 설명하는 "가이드라인"을 fine-tuning된 모델(Gemma3)으로 생성한다.

     ex: 달과 일을 의미하는 int형 변수 m, d를 선언하세요.

  4. 초보자는 해당 가이드라인에 따라 단위별로 코드를 작성한다.

  5. OJ는 초보자가 작성한 코드마다 참, 거짓 여부를 피드백으로 제공한다.

---

### 모델 학습 계획

- 모델 정보
  - 하드웨어 사양 중 가장 성능이 높다고 평가되는 **Gemma3의 4B 모델**을 선정
  - unsloth에서 양자화 + lora를 적용하여 Colab의 무료 버전 위에서 돌아갈 수 있도록 만든 환경을 이용
- 데이터셋 정보
  - **백준에서 Accepted를 받은 올바른 코드와 문제 약 800개를 수집**
  - 데이터의 설명문을 생성하여 **(x, y) = (전처리된 코드, 설명문)** 형태로 구축

---

### Step2 단락 단위 분리 테스트

- 날짜: 2025-04-08
- epoch 50

- 대체로 분류는 잘 되어있음
- 그러나, 설명이 불친절함
- 비교적 긴 문제를 추론하는데 2분 소요

[comjke33/gemma3-4b-2step-fine-tuned · Hugging Face](https://huggingface.co/comjke33/gemma3-4b-2step-fine-tuned)

- 위 huggingface에 업로드함

---

### Step1 줄 단위 분리 테스트

- 날짜: 2025-04-10

- 아래 huggingface에 업로드함

  [gemma3-4b-1step-fine-tuned](https://huggingface.co/comjke33/gemma3-4b-1step-fine-tuned)

---

### Step2 분리 모델 재학습

- 날짜: 2025-04-11

- lora의 Adapter 모델을 huggingface 상에 저장하지 않았다는 사실을 확인.

- Adapter 가중치 행렬을 저장할 수 있도록 다시 huggingface에 업로드하였다.

- 아래의 huggingface에 업로드함

  [gemma-3-4b-1step-lora](https://huggingface.co/comjke33/gemma-3-4b-1step-lora)

---

### Step1 분리 모델 재학습

- 날짜: 2025-04-12

- 데이터셋의 설명문이 너무 친절하지 못하고, 학생이 전체 코드의 흐름을 파악하면서 코드를 작성할 수 없도록 되어있음

- 따라서 코드 데이터셋에서 **설명문을 더 자세하게 수정**함.

- 수정된 데이터셋으로 재학습시키고 lora Adapter 모델을 아래의 huggingface에 업로드함

  [gemma-3-4b-2step-lora](https://huggingface.co/comjke33/gemma-3-4b-2step-lora)