---
layout: post
title:  "04-04 데이터셋자동생성&모델추론테스트"
date:   2025-04-04 10:00:00 +0900
---

- **참석자**: 장경은
- **조사 부분**: 데이터셋자동생성&모델추론테스트

---

# 데이터셋 자동 생성 & 모델 추론 테스트

태그: 장경은
시작 날짜: 2025년 4월 4일

# (완료)데이터셋 생성 코드 작성

### requirements.txt

- code
    
    ```python
    aiohappyeyeballs==2.6.1
    aiohttp==3.11.16
    aiosignal==1.3.2
    annotated-types==0.7.0
    anyio==4.9.0
    attrs==25.3.0
    certifi==2025.1.31
    charset-normalizer==3.4.1
    colorama==0.4.6
    distro==1.9.0
    faiss-cpu==1.10.0
    filelock==3.18.0
    flake8==7.2.0
    flake8-bugbear==24.12.12
    frozenlist==1.5.0
    fsspec==2025.3.2
    h11==0.14.0
    httpcore==1.0.7
    httpx==0.28.1
    huggingface-hub==0.30.1
    idna==3.10
    Jinja2==3.1.6
    jiter==0.9.0
    jsonlines==4.0.0
    jsonschema==4.23.0
    jsonschema-specifications==2024.10.1
    MarkupSafe==3.0.2
    mccabe==0.7.0
    mpmath==1.3.0
    multidict==6.3.2
    networkx==3.4.2
    numpy==2.2.4
    openai==1.70.0
    packaging==24.2
    propcache==0.3.1
    pycodestyle==2.13.0
    pydantic==2.11.2
    pydantic_core==2.33.1
    pyflakes==3.3.2
    PyYAML==6.0.2
    referencing==0.36.2
    regex==2024.11.6
    requests==2.32.3
    rpds-py==0.24.0
    safetensors==0.5.3
    sniffio==1.3.1
    sympy==1.13.1
    tokenizers==0.21.1
    torch==2.6.0
    tqdm==4.67.1
    transformers==4.50.3
    typing-inspection==0.4.0
    typing_extensions==4.13.1
    urllib3==2.3.0
    yarl==1.18.3
    
    ```
    

[0404_requirements.txt](https://github.com/user-attachments/files/19630618/0404_requirements.txt)
### code_similarity_check.py

- code
    
    ```python
    from transformers import RobertaTokenizer, RobertaModel
    import torch
    import numpy as np
    import faiss
    import pickle
    import os
    
    # 1. CodeBERT 로딩
    MODEL_NAME = "microsoft/codebert-base"
    tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)
    model = RobertaModel.from_pretrained(MODEL_NAME)
    
    # 2. 임베딩 추출 함수
    def get_code_embedding(code: str) -> np.ndarray:
        inputs = tokenizer(code, return_tensors="pt", truncation=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs)
        cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS]
        return cls_embedding.squeeze().numpy()
    
    # 3. FAISS 인덱스 초기화 (768차원, L2 거리 기준)
    def index_init(index_name):
        index = faiss.IndexFlatL2(768)
        faiss.write_index(index, index_name +".faiss")
    
    # 4. 기존 코드 저장 및 인덱싱
    def add_to_index(code: str, index):
        vec = get_code_embedding(code).astype('float32')
        index.add(np.array([vec]))  # FAISS에 추가
    
    # 5. 유사도 비교 함수
    def is_similar(code: str, index, threshold=0.95) -> bool:
        vec = get_code_embedding(code).astype('float32').reshape(1, -1)
        if index.ntotal == 0:
            return False
        D, I = index.search(vec, k=1)  # 가장 가까운 코드와의 거리 검색
        similarity = 1 - D[0][0] / 2  # L2 거리 → cosine 유사도 근사
        return similarity > threshold
    
    def check_similarity(index_name, code):
    
        # 새로운 log 파일이라면 index 파일 새로 생성
        if not os.path.exists(index_name+".faiss"):
            index_init(index_name)
            print("새로운 index 생성: ", index_name)
    
        # index load
        index = faiss.read_index(index_name+".faiss")
        print("인덱스 로드 완료: ", index_name)
    
        # 기존에 있는 코드끼리 유사도 판정
        if is_similar(code, index):
            return 1 # 유사한 코드가 있음음
        else:
            # 인덱스 추가
            add_to_index(code, index)
    
            if not os.path.exists(index_name+".pkl"):
                with open(index_name + ".pkl", "wb") as f:
                    pickle.dump([], f)
    
            # code list 배열에 저장 (파일)
            with open(index_name + ".pkl", "r+b") as f:
                loaded_array = pickle.load(f)
                new_array = loaded_array.append(code)
    
                f.seek(0)  # 파일 포인터를 처음으로 이동
                pickle.dump(loaded_array, f)
                f.truncate()
    
            return 0 # 유사한 코드가 없음음
    
    ```
    

[code_similarity_check.py](code_similarity_check.py)

### count_dataset.py

- code
    
    ```python
    import json
    
    # JSON 파일 열기
    with open('dataset.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # 배열의 길이 출력 (최상위 배열의 개수)
    print("배열의 개수:", len(data))
    
    ```
    

[count_dataset.py](count_dataset.py)

### make-dataset-with-gpt.py

- code
    
    ```python
    # -*- coding: utf-8 -*-
    
    from openai import OpenAI
    import code_similarity_check as csc
    import json
    import os
    import re
    from tqdm import tqdm
    
    client = OpenAI(api_key="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")
    
    prompt = """
    C 언어 코드를 분석하여 한 줄씩 설명하고, 특정 형식의 태그를 적용해줘.
    
    설명 및 요구사항  
    1. C 언어 코드를 제공하면, 코드의 흐름을 한 줄씩 분석하여 각 단계별 설명을 작성해줘.  
    2. 코드 블록(함수, 조건문, 반복문 등)의 시작과 끝을 특정 태그로 감싸서 표현해줘.  
    3. 태그 규칙:  
       - 함수: [func_def_start(중첩 레벨)] ~ [func_def_end(중첩 레벨)]  
       - 조건문: [cond_start(중첩 레벨)] ~ [cond_end(중첩 레벨)]  
       - 반복문: [rep_start(중첩 레벨)] ~ [rep_end(중첩 레벨)]  
       - 구조체: [struct_start(중첩 레벨)] ~ [struct_end(중첩 레벨)]  
    4. 태그 안의 숫자(중첩 레벨)는 현재 중첩된 깊이를 의미함.  
       - 예를 들어, for문 안에 또 다른 for문이 있다면,  
         - 바깥 for문 → [rep_start(1)]  
         - 안쪽 for문 → [rep_start(2)]  
         - 안쪽 for문 종료 → [rep_end(2)]  
         - 바깥 for문 종료 → [rep_end(1)]  
    
    예제 코드 & 원하는 출력 예시  
    
    입력 코드:
    c
    #include <stdio.h>
    
    int is_prime(int num) {
        if (num < 2) return 0;
        for (int j = 2; j < num; j++) {
            if (num % j == 0) {
                return 0;
            }
        }
        return 1;
    }
    
    int main(void) {
        int M, N;
        scanf("%d\n%d", &M, &N);
    
        int sum = 0, min = 0;
        for (int i = M; i <= N; i++) {
            if (is_prime(i)) {
                sum += i;
                if (min == 0) {
                    min = i;
                }
            }
        }
        if (min != 0)
            printf("%d\n%d", sum, min);
        else
            printf("-1");
    
        return 0;
    }
    출력 예시:
    
    1. [func_def_start(1)] int 반환형 함수 is_prime을 정의하세요. (매개변수: int num)
    2. [cond_start(1)] num이 2보다 작은지 확인하는 조건문을 작성하세요.
    3. num이 2보다 작다면 0을 반환하세요.
    4. [cond_end(1)]
    5. [rep_start(1)] j를 선언해 2부터 num-1까지 반복하는 for 루프를 작성하세요.
    6. [cond_start(2)] num이 j로 나누어지는지 확인하는 조건문을 작성하세요.
    7. num이 j로 나누어지면 0을 반환하세요.
    8. [cond_end(2)]
    9. [rep_end(1)]
    10. 위의 과정을 통과한 num은 소수이므로 1을 반환하세요.
    11. [func_def_end(1)]
    12. [func_def_start(1)] int 반환형 main 함수를 정의하세요. (매개변수 없음)
    13. 정수형 변수 M과 N을 선언하세요.
    14. M과 N에 입력을 받으세요.
    15. 소수의 합을 저장할 int형 변수 sum을 0으로 초기화하세요.
    16. 첫 번째 소수를 저장할 int형 변수 min을 0으로 초기화하세요.
    17. [rep_start(1)] i를 선언해 M부터 N까지 반복하는 for 루프를 작성하세요.
    18. [cond_start(2)] is_prime 함수를 이용하여 i가 소수인지 판별하는 조건문을 작성하세요.
    19. i가 소수라면 sum에 i를 더하세요.
    20. [cond_start(3)] 이전에 소수가 나왔는지 확인하는 조건문을 작성하세요.
    21. 이전에 소수가 나오지 않았다면 첫 번째 소수의 값을 i로 설정하세요.
    22. [cond_end(3)]
    23. [cond_end(2)]
    24. [rep_end(1)]
    25. [cond_start(1)] 소수가 존재하는지 확인하는 조건문을 작성하세요.
    26. 소수가 존재하면 sum과 min을 출력하세요.
    27. [cond_end(1)]
    28. [cond_start(1)] 소수가 존재하지 않는 경우를 처리하는 조건문을 작성하세요.
    29. 소수가 없으면 -1을 출력하세요.
    30. [cond_end(1)]
    31. return 0;으로 프로그램을 종료하세요.
    32. [func_def_end(1)]
    이제 제공하는 C 코드도 이 형식으로 변환해서 설명해줘.
    """
    
    def remove_c_comments(code: str) -> str:
        # 여러 줄 주석 제거: /* ... */
        code = re.sub(r'/\*.*?\*/', '', code, flags=re.DOTALL)
    
        # 한 줄 주석 제거: // ...
        code = re.sub(r'//.*', '', code)
    
        return code
    
    def parse_log_file(filename):
        with open(filename, 'r', encoding='utf-8') as file:
            log_text = file.read()
    
        # 로그를 줄 단위로 구분
        log_entries = log_text.strip().split("------------------------------------------------------")
    
        parsed_data = []
        
        for entry in log_entries:
            lines = entry.strip().split("\n")
            if lines and len(lines) > 1:
                # 헤더 파싱
                header = lines[0].strip()
                id_, problem, result = header.split(":", 2)
                problem_number_str = ''.join(filter(str.isdigit, problem))
                # 코드 부분 파싱
                code = "\n".join(lines[1:]).strip()
                code = remove_c_comments(code)
                # code_lines = code.splitlines()
                # filtered_lines = [line for line in code_lines if "#define _CRT_SECURE_NO_WARNINGS" not in line]
                # code = '\n'.join(filtered_lines)  
                # 파싱된 데이터를 리스트에 저장
                parsed_data.append({
                    'id': id_,
                    'problem': int(problem_number_str),
                    'result': result,
                    'code': code
                })
        
        return parsed_data
    
    cnt = 0
    
    for i in tqdm(range(1000, 1025)):
    
        log_filename =  "logs-" + str(i)
    
        # 로그 파일 파싱
        parsed_data_list = parse_log_file('./log/'+log_filename + '.txt')
    
        for i in tqdm(range(len(parsed_data_list))):
            parsed_data = parsed_data_list[i]
    
            cnt = cnt + 1
            if cnt > 100:
                break
            if parsed_data['result'] == 'ACCEPTED':
                # print(parsed_data['result'])
                # tap = input()  
                # print(parsed_data['code'])
                # print("------------------------------------------------------")
                code = parsed_data['code']
    
                if csc.check_similarity(log_filename, code):
                    continue
                
                response = client.responses.create(
                  model="gpt-4o-mini-2024-07-18",
                  input = prompt + "\n"+ code 
                )
    
                print(response.output_text)
    
                # 새로 추가할 항목 정의
                new_entry = {
                    "conversations": [
                        {"role": "user", "content": code},
                        {"role": "assistant", "content": response.output_text}
                    ],
                    "source": "custom-dataset",
                    "score": 4.8
                }
    
                # 파일 경로
                file_path = "dataset.json"
    
                # 기존 데이터 불러오기
                if os.path.exists(file_path):
                    with open(file_path, "r", encoding="utf-8") as f:
                        try:
                            data = json.load(f)
                        except json.JSONDecodeError:
                            data = []
                else:
                    data = []
    
                # 새 항목 추가
                data.append(new_entry)
    
                # JSON으로 저장 (줄바꿈은 \n으로 저장됨)
                with open(file_path, "w", encoding="utf-8") as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)
    
                print("dataset.json에 저장되었습니다.")
    
    ```
    

[make-dataset-with-gpt.py](make-dataset-with-gpt.py)

# 데이터셋 모음

[0404_logs_1000.json](https://github.com/user-attachments/files/19630619/0404_logs_1000.json)

- logs-1000.txt
- 109개

[0404_logs_1001.json](https://github.com/user-attachments/files/19630616/0404_logs_1001.json)

- logs-1001.txt
- 95개

[0404_logs_1002.json](https://github.com/user-attachments/files/19630614/0404_logs_1002.json)

- logs-1002.txt
- 105개

[0404_logs_1003.json](https://github.com/user-attachments/files/19630617/0404_logs_1003.json)

- logs-1003.txt
- 155개

[0404_logs_1004.json](https://github.com/user-attachments/files/19630613/0404_logs_1004.json)

- logs-1004.txt
- 114개

### logs-1000.txt ~ logs-1004.txt

[0404_1000-1004.json](https://github.com/user-attachments/files/19630615/0404_1000-1004.json)

- 574개

[0404_yunho_dataset.json](https://github.com/user-attachments/files/19630597/0404_yunho_dataset.json)

- 연호님 데이터셋
- 110개

# 모델 fine-tuning

[https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/HuggingFace Course-Gemma3_(1B)-GRPO.ipynb](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/HuggingFace%20Course-Gemma3_(1B)-GRPO.ipynb)
